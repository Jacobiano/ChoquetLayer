{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49f1e2c1-b6cd-4fb0-908f-87b1ccee6122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n",
      "listIm.shape (4048, 128, 128)\n",
      "listImVal.shape (512, 128, 128)\n",
      "listY.shape (4048, 1)\n",
      "listYVal.shape (512, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Layer\n",
    "from skimage.io import imread\n",
    "from shutil import copyfile\n",
    "#import tensorflow_probability as tfp\n",
    "print('Tensorflow version')\n",
    "print(tf.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.gray()\n",
    "from sklearn.metrics import accuracy_score\n",
    "#import tensorflow_probability as tfp\n",
    "import random\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "\n",
    "BATCH_SIZE = int(32.)\n",
    "EPOCHS = int(512.)\n",
    "\n",
    "batch_size = BATCH_SIZE\n",
    "epochs = EPOCHS\n",
    "learning_rate=0.001\n",
    "CHANNELS=1\n",
    "NLAYERS=3\n",
    "SHRINK=1\n",
    "NFILTERS=48\n",
    "KSIZE=13\n",
    "SUBSPACE=12\n",
    "PATIENCE_ES=40\n",
    "PATIENCE_RP=5\n",
    "\n",
    "\n",
    "class NeymanScott:\n",
    "    \"\"\"\n",
    "    Neyman-Scott point process using a Poisson variable for the number of parent points, uniform for\n",
    "    the number of daughter points and Pareto distribution for the distance from the daughter points to\n",
    "    the parent.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 poisson_mean: float,\n",
    "                 daughter_max: int,\n",
    "                 pareto_alpha: float,\n",
    "                 pareto_scale: float,\n",
    "                 size: (int, int)):\n",
    "        \"\"\"\n",
    "        :param poisson_mean: mean of the number of parent points\n",
    "        :param daughter_max: maximum number of daughters per parent points\n",
    "        :param pareto_alpha: alpha parameter of the Pareto distribution\n",
    "        :param pareto_scale: scale used in the Pareto distribution. This parameter is\n",
    "            applied before resizing the points from the [0, 1] interval to the size of the image.\n",
    "        :param size: rescale the output to this size\n",
    "        \"\"\"\n",
    "        self.poisson_mean = poisson_mean\n",
    "        self.daughter_max = daughter_max\n",
    "        self.pareto_alpha = pareto_alpha\n",
    "        self.pareto_scale = pareto_scale\n",
    "        self.size = np.array([size])\n",
    "        self.generator = np.random.Generator(np.random.PCG64())\n",
    "\n",
    "    def __call__(self):\n",
    "        num_parents = self.generator.poisson(lam=self.poisson_mean)\n",
    "        parents = self.generator.random((num_parents, 2))\n",
    "        num_daughters = self.generator.integers(1, self.daughter_max, num_parents)\n",
    "        points = np.empty((0, 2))\n",
    "\n",
    "        for i in range(num_parents):\n",
    "            # normalizes the pareto II distribution\n",
    "            dist = self.generator.pareto(self.pareto_alpha, (num_daughters[i], 1))\n",
    "            dist = (dist + 1) * self.pareto_scale\n",
    "            angle = self.generator.uniform(0., 2 * np.pi, (num_daughters[i],))\n",
    "            positions = np.stack([np.cos(angle), np.sin(angle)], 1)\n",
    "            positions *= dist\n",
    "            positions += parents[i, np.newaxis, :]\n",
    "            points = np.concatenate([points, positions])\n",
    "        # remove points outside the set [0, 1] x [0, 1]\n",
    "        valid_points = np.logical_and(\n",
    "            np.logical_and(0. <= points[:, 0], points[:, 0] <= 1.),\n",
    "            np.logical_and(0. <= points[:, 1], points[:, 1] <= 1.)\n",
    "        )\n",
    "        points = points[valid_points, :]\n",
    "        # scale to the image size\n",
    "        points = points * self.size\n",
    "        return points\n",
    "\n",
    "\n",
    "\n",
    "NSAMPLES_TRAINING=2024*2\n",
    "IMG_SIZE=128\n",
    "\n",
    "poisson_mean=100\n",
    "daughter_max=50\n",
    "pareto_scale=.02\n",
    "pareto_alpha=1. #GENERATION ON IT\n",
    "gen = NeymanScott(poisson_mean, daughter_max, pareto_alpha, pareto_scale, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "generatedata=1\n",
    "if generatedata==1:\n",
    "    listIm=[]\n",
    "    listY=[]\n",
    "    for i in range(NSAMPLES_TRAINING):\n",
    "        pareto_alpha=gen.generator.random(1)*10\n",
    "        gen = NeymanScott(poisson_mean, daughter_max, pareto_alpha, pareto_scale, (IMG_SIZE, IMG_SIZE))\n",
    "        points = gen()\n",
    "        I=np.zeros([IMG_SIZE,IMG_SIZE])\n",
    "        I[np.int64(np.floor(points[:, 0])), np.int64(np.floor(points[:, 1]))]=1\n",
    "        listIm.append(I)\n",
    "        listY.append(pareto_alpha)\n",
    "    listIm=np.stack(listIm)\n",
    "    listY=np.stack(listY)\n",
    "\n",
    "    NSAMPLES_VALIDATION=512\n",
    "    listImVal=[]\n",
    "    listYVal=[]\n",
    "    for i in range(NSAMPLES_VALIDATION):\n",
    "        pareto_alpha=gen.generator.random(1)*10\n",
    "        gen = NeymanScott(poisson_mean, daughter_max, pareto_alpha, pareto_scale, (IMG_SIZE, IMG_SIZE))\n",
    "        points = gen()\n",
    "        I=np.zeros([IMG_SIZE,IMG_SIZE])\n",
    "        I[np.int64(np.floor(points[:, 0])), np.int64(np.floor(points[:, 1]))]=1\n",
    "        listImVal.append(I)\n",
    "        listYVal.append(pareto_alpha)\n",
    "\n",
    "    listImVal=np.stack(listImVal)\n",
    "    listYVal=np.stack(listYVal)\n",
    "    np.save('listIm.npy',listIm)\n",
    "    np.save('listImVal.npy',listImVal)\n",
    "    np.save('listY.npy',listY)\n",
    "    np.save('listYVal.npy',listYVal)\n",
    "else:\n",
    "    listIm=np.load('listIm.npy')\n",
    "    listImVal=np.load('listImVal.npy')\n",
    "    listY=np.load('listY.npy')\n",
    "    listYVal=np.load('listYVal.npy')\n",
    "\n",
    "print('listIm.shape',listIm.shape)\n",
    "print('listImVal.shape',listImVal.shape)\n",
    "print('listY.shape',listY.shape)\n",
    "print('listYVal.shape',listYVal.shape)\n",
    "\n",
    "listY=listY/9\n",
    "listYVal=listYVal/9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fdff72-ce5c-4e78-b214-887827dc0df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 09:33:42.438256: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Max\n",
      "2023-10-06 09:33:42.438271: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2023-10-06 09:33:42.438276: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2023-10-06 09:33:42.438302: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-06 09:33:42.438314: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 128, 128, 1)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 128, 128, 48)      480       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 128, 128, 48)      20784     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 128, 128, 48)      20784     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 128, 128, 48)      2352      \n",
      "                                                                 \n",
      " depthwise_conv2d (Depthwis  (None, 128, 128, 48)      8160      \n",
      " eConv2D)                                                        \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 48)                0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 48)                192       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 12)                588       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 12)                156       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53509 (209.02 KB)\n",
      "Trainable params: 53413 (208.64 KB)\n",
      "Non-trainable params: 96 (384.00 Byte)\n",
      "_________________________________________________________________\n",
      "53509\n",
      "Epoch 1/512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 09:33:44.019041: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/127 [==============================] - ETA: 0s - loss: 0.1239 - mse: 0.0276 - mae: 0.1239"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 09:34:19.974059: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/127 [==============================] - 38s 267ms/step - loss: 0.1239 - mse: 0.0276 - mae: 0.1239 - val_loss: 0.5129 - val_mse: 0.3600 - val_mae: 0.5129 - lr: 0.0010\n",
      "Epoch 2/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.1029 - mse: 0.0180 - mae: 0.1029 - val_loss: 0.3968 - val_mse: 0.2295 - val_mae: 0.3968 - lr: 0.0010\n",
      "Epoch 3/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.1008 - mse: 0.0171 - mae: 0.1008 - val_loss: 0.5449 - val_mse: 0.3999 - val_mae: 0.5449 - lr: 0.0010\n",
      "Epoch 4/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.1021 - mse: 0.0174 - mae: 0.1021 - val_loss: 0.5428 - val_mse: 0.3949 - val_mae: 0.5428 - lr: 0.0010\n",
      "Epoch 5/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0949 - mse: 0.0142 - mae: 0.0949 - val_loss: 0.4747 - val_mse: 0.2941 - val_mae: 0.4747 - lr: 0.0010\n",
      "Epoch 6/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0876 - mse: 0.0122 - mae: 0.0876 - val_loss: 0.1428 - val_mse: 0.0277 - val_mae: 0.1428 - lr: 0.0010\n",
      "Epoch 7/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0905 - mse: 0.0132 - mae: 0.0905 - val_loss: 0.3014 - val_mse: 0.1183 - val_mae: 0.3014 - lr: 0.0010\n",
      "Epoch 8/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0853 - mse: 0.0117 - mae: 0.0853 - val_loss: 0.3098 - val_mse: 0.1186 - val_mae: 0.3098 - lr: 0.0010\n",
      "Epoch 9/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0837 - mse: 0.0112 - mae: 0.0837 - val_loss: 0.5498 - val_mse: 0.4074 - val_mae: 0.5498 - lr: 0.0010\n",
      "Epoch 10/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0855 - mse: 0.0115 - mae: 0.0855 - val_loss: 0.2745 - val_mse: 0.1035 - val_mae: 0.2745 - lr: 0.0010\n",
      "Epoch 11/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0889 - mse: 0.0128 - mae: 0.0889 - val_loss: 0.3420 - val_mse: 0.1575 - val_mae: 0.3420 - lr: 0.0010\n",
      "Epoch 12/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0770 - mse: 0.0095 - mae: 0.0770 - val_loss: 0.2731 - val_mse: 0.0957 - val_mae: 0.2731 - lr: 1.0000e-04\n",
      "Epoch 13/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0783 - mse: 0.0099 - mae: 0.0783 - val_loss: 0.0820 - val_mse: 0.0105 - val_mae: 0.0820 - lr: 1.0000e-04\n",
      "Epoch 14/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0772 - mse: 0.0096 - mae: 0.0772 - val_loss: 0.1105 - val_mse: 0.0172 - val_mae: 0.1105 - lr: 1.0000e-04\n",
      "Epoch 15/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0754 - mse: 0.0093 - mae: 0.0754 - val_loss: 0.1565 - val_mse: 0.0325 - val_mae: 0.1565 - lr: 1.0000e-04\n",
      "Epoch 16/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0733 - mse: 0.0088 - mae: 0.0733 - val_loss: 0.0826 - val_mse: 0.0113 - val_mae: 0.0826 - lr: 1.0000e-04\n",
      "Epoch 17/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0783 - mse: 0.0098 - mae: 0.0783 - val_loss: 0.1286 - val_mse: 0.0233 - val_mae: 0.1286 - lr: 1.0000e-04\n",
      "Epoch 18/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0780 - mse: 0.0098 - mae: 0.0780 - val_loss: 0.1266 - val_mse: 0.0228 - val_mae: 0.1266 - lr: 1.0000e-04\n",
      "Epoch 19/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0714 - mse: 0.0086 - mae: 0.0714 - val_loss: 0.0684 - val_mse: 0.0077 - val_mae: 0.0684 - lr: 1.0000e-05\n",
      "Epoch 20/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0727 - mse: 0.0087 - mae: 0.0727 - val_loss: 0.0636 - val_mse: 0.0066 - val_mae: 0.0636 - lr: 1.0000e-05\n",
      "Epoch 21/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0753 - mse: 0.0093 - mae: 0.0753 - val_loss: 0.0606 - val_mse: 0.0061 - val_mae: 0.0606 - lr: 1.0000e-05\n",
      "Epoch 22/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0728 - mse: 0.0088 - mae: 0.0728 - val_loss: 0.0607 - val_mse: 0.0061 - val_mae: 0.0607 - lr: 1.0000e-05\n",
      "Epoch 23/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0727 - mse: 0.0087 - mae: 0.0727 - val_loss: 0.0613 - val_mse: 0.0063 - val_mae: 0.0613 - lr: 1.0000e-05\n",
      "Epoch 24/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0753 - mse: 0.0092 - mae: 0.0753 - val_loss: 0.0605 - val_mse: 0.0061 - val_mae: 0.0605 - lr: 1.0000e-05\n",
      "Epoch 25/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0738 - mse: 0.0089 - mae: 0.0738 - val_loss: 0.0605 - val_mse: 0.0061 - val_mae: 0.0605 - lr: 1.0000e-05\n",
      "Epoch 26/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0748 - mse: 0.0092 - mae: 0.0748 - val_loss: 0.0608 - val_mse: 0.0062 - val_mae: 0.0608 - lr: 1.0000e-05\n",
      "Epoch 27/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0719 - mse: 0.0087 - mae: 0.0719 - val_loss: 0.0603 - val_mse: 0.0061 - val_mae: 0.0603 - lr: 1.0000e-05\n",
      "Epoch 28/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0740 - mse: 0.0090 - mae: 0.0740 - val_loss: 0.0610 - val_mse: 0.0062 - val_mae: 0.0610 - lr: 1.0000e-05\n",
      "Epoch 29/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0724 - mse: 0.0086 - mae: 0.0724 - val_loss: 0.0618 - val_mse: 0.0065 - val_mae: 0.0618 - lr: 1.0000e-05\n",
      "Epoch 30/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0723 - mse: 0.0086 - mae: 0.0723 - val_loss: 0.0604 - val_mse: 0.0061 - val_mae: 0.0604 - lr: 1.0000e-05\n",
      "Epoch 31/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0731 - mse: 0.0088 - mae: 0.0731 - val_loss: 0.0608 - val_mse: 0.0063 - val_mae: 0.0608 - lr: 1.0000e-05\n",
      "Epoch 32/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0744 - mse: 0.0090 - mae: 0.0744 - val_loss: 0.0608 - val_mse: 0.0061 - val_mae: 0.0608 - lr: 1.0000e-05\n",
      "Epoch 33/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0755 - mse: 0.0094 - mae: 0.0755 - val_loss: 0.0605 - val_mse: 0.0061 - val_mae: 0.0605 - lr: 1.0000e-06\n",
      "Epoch 34/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0733 - mse: 0.0089 - mae: 0.0733 - val_loss: 0.0607 - val_mse: 0.0061 - val_mae: 0.0607 - lr: 1.0000e-06\n",
      "Epoch 35/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0734 - mse: 0.0088 - mae: 0.0734 - val_loss: 0.0606 - val_mse: 0.0061 - val_mae: 0.0606 - lr: 1.0000e-06\n",
      "Epoch 36/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0712 - mse: 0.0085 - mae: 0.0712 - val_loss: 0.0604 - val_mse: 0.0061 - val_mae: 0.0604 - lr: 1.0000e-06\n",
      "Epoch 37/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0732 - mse: 0.0087 - mae: 0.0732 - val_loss: 0.0607 - val_mse: 0.0062 - val_mae: 0.0607 - lr: 1.0000e-06\n",
      "Epoch 38/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0749 - mse: 0.0091 - mae: 0.0749 - val_loss: 0.0607 - val_mse: 0.0062 - val_mae: 0.0607 - lr: 1.0000e-06\n",
      "Epoch 39/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0755 - mse: 0.0093 - mae: 0.0755 - val_loss: 0.0607 - val_mse: 0.0061 - val_mae: 0.0607 - lr: 1.0000e-06\n",
      "Epoch 40/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0731 - mse: 0.0089 - mae: 0.0731 - val_loss: 0.0606 - val_mse: 0.0061 - val_mae: 0.0606 - lr: 1.0000e-06\n",
      "Epoch 41/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0731 - mse: 0.0087 - mae: 0.0731 - val_loss: 0.0605 - val_mse: 0.0061 - val_mae: 0.0605 - lr: 1.0000e-06\n",
      "Epoch 42/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0704 - mse: 0.0083 - mae: 0.0704 - val_loss: 0.0604 - val_mse: 0.0061 - val_mae: 0.0604 - lr: 1.0000e-06\n",
      "Epoch 43/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0734 - mse: 0.0088 - mae: 0.0734 - val_loss: 0.0604 - val_mse: 0.0061 - val_mae: 0.0604 - lr: 1.0000e-06\n",
      "Epoch 44/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0729 - mse: 0.0089 - mae: 0.0729 - val_loss: 0.0606 - val_mse: 0.0061 - val_mae: 0.0606 - lr: 1.0000e-06\n",
      "Epoch 45/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0728 - mse: 0.0089 - mae: 0.0728 - val_loss: 0.0604 - val_mse: 0.0061 - val_mae: 0.0604 - lr: 1.0000e-06\n",
      "Epoch 46/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0705 - mse: 0.0082 - mae: 0.0705 - val_loss: 0.0605 - val_mse: 0.0061 - val_mae: 0.0605 - lr: 1.0000e-06\n",
      "Epoch 47/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0715 - mse: 0.0085 - mae: 0.0715 - val_loss: 0.0605 - val_mse: 0.0061 - val_mae: 0.0605 - lr: 1.0000e-06\n",
      "Epoch 48/512\n",
      "127/127 [==============================] - 1030s 8s/step - loss: 0.0759 - mse: 0.0094 - mae: 0.0759 - val_loss: 0.0605 - val_mse: 0.0061 - val_mae: 0.0605 - lr: 1.0000e-06\n",
      "Epoch 49/512\n",
      "127/127 [==============================] - 980s 8s/step - loss: 0.0739 - mse: 0.0090 - mae: 0.0739 - val_loss: 0.0605 - val_mse: 0.0061 - val_mae: 0.0605 - lr: 1.0000e-06\n",
      "Epoch 50/512\n",
      "127/127 [==============================] - 31s 244ms/step - loss: 0.0725 - mse: 0.0087 - mae: 0.0725 - val_loss: 0.0607 - val_mse: 0.0062 - val_mae: 0.0607 - lr: 1.0000e-06\n",
      "Epoch 51/512\n",
      "127/127 [==============================] - 31s 245ms/step - loss: 0.0712 - mse: 0.0084 - mae: 0.0712 - val_loss: 0.0605 - val_mse: 0.0061 - val_mae: 0.0605 - lr: 1.0000e-06\n",
      "Epoch 52/512\n",
      "127/127 [==============================] - 31s 245ms/step - loss: 0.0736 - mse: 0.0089 - mae: 0.0736 - val_loss: 0.0604 - val_mse: 0.0061 - val_mae: 0.0604 - lr: 1.0000e-06\n",
      "Epoch 53/512\n",
      "127/127 [==============================] - 31s 245ms/step - loss: 0.0731 - mse: 0.0088 - mae: 0.0731 - val_loss: 0.0605 - val_mse: 0.0061 - val_mae: 0.0605 - lr: 1.0000e-06\n",
      "Epoch 54/512\n",
      "127/127 [==============================] - 35s 276ms/step - loss: 0.0709 - mse: 0.0084 - mae: 0.0709 - val_loss: 0.0604 - val_mse: 0.0061 - val_mae: 0.0604 - lr: 1.0000e-06\n",
      "Epoch 55/512\n",
      "127/127 [==============================] - 953s 8s/step - loss: 0.0711 - mse: 0.0083 - mae: 0.0711 - val_loss: 0.0605 - val_mse: 0.0061 - val_mae: 0.0605 - lr: 1.0000e-06\n",
      "Epoch 56/512\n",
      "127/127 [==============================] - 31s 245ms/step - loss: 0.0746 - mse: 0.0090 - mae: 0.0746 - val_loss: 0.0606 - val_mse: 0.0061 - val_mae: 0.0606 - lr: 1.0000e-06\n",
      "Epoch 57/512\n",
      "127/127 [==============================] - 31s 245ms/step - loss: 0.0742 - mse: 0.0090 - mae: 0.0742 - val_loss: 0.0606 - val_mse: 0.0061 - val_mae: 0.0606 - lr: 1.0000e-06\n",
      "Epoch 58/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0714 - mse: 0.0084 - mae: 0.0714 - val_loss: 0.0606 - val_mse: 0.0061 - val_mae: 0.0606 - lr: 1.0000e-06\n",
      "Epoch 59/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0717 - mse: 0.0087 - mae: 0.0717 - val_loss: 0.0606 - val_mse: 0.0061 - val_mae: 0.0606 - lr: 1.0000e-06\n",
      "Epoch 60/512\n",
      "127/127 [==============================] - 36s 283ms/step - loss: 0.0756 - mse: 0.0094 - mae: 0.0756 - val_loss: 0.0606 - val_mse: 0.0061 - val_mae: 0.0606 - lr: 1.0000e-06\n",
      "Epoch 61/512\n",
      "127/127 [==============================] - 897s 7s/step - loss: 0.0727 - mse: 0.0086 - mae: 0.0727 - val_loss: 0.0607 - val_mse: 0.0061 - val_mae: 0.0607 - lr: 1.0000e-06\n",
      "Epoch 62/512\n",
      "127/127 [==============================] - 31s 246ms/step - loss: 0.0735 - mse: 0.0090 - mae: 0.0735 - val_loss: 0.0607 - val_mse: 0.0061 - val_mae: 0.0607 - lr: 1.0000e-06\n",
      "Epoch 63/512\n",
      "127/127 [==============================] - 31s 247ms/step - loss: 0.0755 - mse: 0.0094 - mae: 0.0755 - val_loss: 0.0607 - val_mse: 0.0061 - val_mae: 0.0607 - lr: 1.0000e-06\n",
      "Epoch 64/512\n",
      "127/127 [==============================] - 31s 247ms/step - loss: 0.0704 - mse: 0.0082 - mae: 0.0704 - val_loss: 0.0605 - val_mse: 0.0061 - val_mae: 0.0605 - lr: 1.0000e-06\n",
      "Epoch 65/512\n",
      "127/127 [==============================] - 31s 247ms/step - loss: 0.0733 - mse: 0.0087 - mae: 0.0733 - val_loss: 0.0605 - val_mse: 0.0061 - val_mae: 0.0605 - lr: 1.0000e-06\n",
      "Epoch 66/512\n",
      "127/127 [==============================] - 31s 247ms/step - loss: 0.0737 - mse: 0.0089 - mae: 0.0737 - val_loss: 0.0606 - val_mse: 0.0061 - val_mae: 0.0606 - lr: 1.0000e-06\n",
      "Epoch 67/512\n",
      "127/127 [==============================] - 31s 247ms/step - loss: 0.0726 - mse: 0.0087 - mae: 0.0726 - val_loss: 0.0606 - val_mse: 0.0061 - val_mae: 0.0606 - lr: 1.0000e-06\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 128, 128, 1)]     0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 128, 128, 48)      480       \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 128, 128, 48)      20784     \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 128, 128, 48)      20784     \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 128, 128, 48)      389376    \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 48)                0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 48)                192       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 12)                588       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 12)                156       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 432373 (1.65 MB)\n",
      "Trainable params: 432277 (1.65 MB)\n",
      "Non-trainable params: 96 (384.00 Byte)\n",
      "_________________________________________________________________\n",
      "432373\n",
      "Epoch 1/512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 11:11:08.179505: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/127 [==============================] - ETA: 0s - loss: 0.1215 - mse: 0.0268 - mae: 0.1215"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 11:11:42.978966: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/127 [==============================] - 37s 267ms/step - loss: 0.1215 - mse: 0.0268 - mae: 0.1215 - val_loss: 0.4439 - val_mse: 0.2704 - val_mae: 0.4439 - lr: 0.0010\n",
      "Epoch 2/512\n",
      "127/127 [==============================] - 32s 250ms/step - loss: 0.1055 - mse: 0.0187 - mae: 0.1055 - val_loss: 0.3489 - val_mse: 0.1747 - val_mae: 0.3489 - lr: 0.0010\n",
      "Epoch 3/512\n",
      "127/127 [==============================] - 32s 249ms/step - loss: 0.1084 - mse: 0.0205 - mae: 0.1084 - val_loss: 0.4963 - val_mse: 0.3310 - val_mae: 0.4963 - lr: 0.0010\n",
      "Epoch 4/512\n",
      "127/127 [==============================] - 32s 249ms/step - loss: 0.1149 - mse: 0.0224 - mae: 0.1149 - val_loss: 0.3783 - val_mse: 0.1728 - val_mae: 0.3783 - lr: 0.0010\n",
      "Epoch 5/512\n",
      "127/127 [==============================] - 32s 250ms/step - loss: 0.1288 - mse: 0.0283 - mae: 0.1288 - val_loss: 0.5509 - val_mse: 0.4092 - val_mae: 0.5509 - lr: 0.0010\n",
      "Epoch 6/512\n",
      "127/127 [==============================] - 32s 249ms/step - loss: 0.1065 - mse: 0.0193 - mae: 0.1065 - val_loss: 0.4646 - val_mse: 0.2711 - val_mae: 0.4646 - lr: 0.0010\n",
      "Epoch 7/512\n",
      "127/127 [==============================] - 32s 249ms/step - loss: 0.1196 - mse: 0.0260 - mae: 0.1196 - val_loss: 0.4443 - val_mse: 0.2806 - val_mae: 0.4443 - lr: 0.0010\n",
      "Epoch 8/512\n",
      "127/127 [==============================] - 32s 249ms/step - loss: 0.0989 - mse: 0.0191 - mae: 0.0989 - val_loss: 0.3676 - val_mse: 0.1849 - val_mae: 0.3676 - lr: 1.0000e-04\n",
      "Epoch 9/512\n",
      "127/127 [==============================] - 32s 249ms/step - loss: 0.0846 - mse: 0.0120 - mae: 0.0846 - val_loss: 0.0755 - val_mse: 0.0096 - val_mae: 0.0755 - lr: 1.0000e-04\n",
      "Epoch 10/512\n",
      "127/127 [==============================] - 32s 249ms/step - loss: 0.0818 - mse: 0.0110 - mae: 0.0818 - val_loss: 0.1290 - val_mse: 0.0225 - val_mae: 0.1290 - lr: 1.0000e-04\n",
      "Epoch 11/512\n",
      "127/127 [==============================] - 32s 249ms/step - loss: 0.0788 - mse: 0.0100 - mae: 0.0788 - val_loss: 0.2443 - val_mse: 0.0737 - val_mae: 0.2443 - lr: 1.0000e-04\n",
      "Epoch 12/512\n",
      "127/127 [==============================] - 32s 249ms/step - loss: 0.0782 - mse: 0.0096 - mae: 0.0782 - val_loss: 0.1812 - val_mse: 0.0411 - val_mae: 0.1812 - lr: 1.0000e-04\n",
      "Epoch 13/512\n",
      "127/127 [==============================] - 32s 249ms/step - loss: 0.0775 - mse: 0.0096 - mae: 0.0775 - val_loss: 0.2604 - val_mse: 0.0800 - val_mae: 0.2604 - lr: 1.0000e-04\n",
      "Epoch 14/512\n",
      "127/127 [==============================] - 963s 8s/step - loss: 0.0759 - mse: 0.0092 - mae: 0.0759 - val_loss: 0.3191 - val_mse: 0.1217 - val_mae: 0.3191 - lr: 1.0000e-04\n",
      "Epoch 15/512\n",
      "127/127 [==============================] - 889s 7s/step - loss: 0.0750 - mse: 0.0091 - mae: 0.0750 - val_loss: 0.1038 - val_mse: 0.0151 - val_mae: 0.1038 - lr: 1.0000e-05\n",
      "Epoch 16/512\n",
      "127/127 [==============================] - 43s 341ms/step - loss: 0.0744 - mse: 0.0091 - mae: 0.0744 - val_loss: 0.0727 - val_mse: 0.0082 - val_mae: 0.0727 - lr: 1.0000e-05\n",
      "Epoch 17/512\n",
      "127/127 [==============================] - 32s 250ms/step - loss: 0.0739 - mse: 0.0089 - mae: 0.0739 - val_loss: 0.0663 - val_mse: 0.0073 - val_mae: 0.0663 - lr: 1.0000e-05\n",
      "Epoch 18/512\n",
      "127/127 [==============================] - 32s 250ms/step - loss: 0.0749 - mse: 0.0091 - mae: 0.0749 - val_loss: 0.0715 - val_mse: 0.0078 - val_mae: 0.0715 - lr: 1.0000e-05\n",
      "Epoch 19/512\n",
      "127/127 [==============================] - 32s 250ms/step - loss: 0.0782 - mse: 0.0099 - mae: 0.0782 - val_loss: 0.0760 - val_mse: 0.0086 - val_mae: 0.0760 - lr: 1.0000e-05\n",
      "Epoch 20/512\n",
      "127/127 [==============================] - 32s 250ms/step - loss: 0.0758 - mse: 0.0093 - mae: 0.0758 - val_loss: 0.0658 - val_mse: 0.0070 - val_mae: 0.0658 - lr: 1.0000e-05\n",
      "Epoch 21/512\n",
      "127/127 [==============================] - 79s 626ms/step - loss: 0.0776 - mse: 0.0096 - mae: 0.0776 - val_loss: 0.0715 - val_mse: 0.0083 - val_mae: 0.0715 - lr: 1.0000e-05\n",
      "Epoch 22/512\n",
      "127/127 [==============================] - 32s 250ms/step - loss: 0.0743 - mse: 0.0090 - mae: 0.0743 - val_loss: 0.0674 - val_mse: 0.0073 - val_mae: 0.0674 - lr: 1.0000e-05\n",
      "Epoch 23/512\n",
      "127/127 [==============================] - 32s 250ms/step - loss: 0.0739 - mse: 0.0089 - mae: 0.0739 - val_loss: 0.0720 - val_mse: 0.0081 - val_mae: 0.0720 - lr: 1.0000e-05\n",
      "Epoch 24/512\n",
      "127/127 [==============================] - 32s 250ms/step - loss: 0.0780 - mse: 0.0099 - mae: 0.0780 - val_loss: 0.0682 - val_mse: 0.0075 - val_mae: 0.0682 - lr: 1.0000e-05\n",
      "Epoch 25/512\n",
      "127/127 [==============================] - 32s 250ms/step - loss: 0.0760 - mse: 0.0092 - mae: 0.0760 - val_loss: 0.0716 - val_mse: 0.0079 - val_mae: 0.0716 - lr: 1.0000e-05\n",
      "Epoch 26/512\n",
      "127/127 [==============================] - 32s 250ms/step - loss: 0.0755 - mse: 0.0092 - mae: 0.0755 - val_loss: 0.0627 - val_mse: 0.0063 - val_mae: 0.0627 - lr: 1.0000e-06\n",
      "Epoch 27/512\n",
      "127/127 [==============================] - 32s 250ms/step - loss: 0.0728 - mse: 0.0087 - mae: 0.0728 - val_loss: 0.0609 - val_mse: 0.0060 - val_mae: 0.0609 - lr: 1.0000e-06\n",
      "Epoch 28/512\n",
      "127/127 [==============================] - 32s 249ms/step - loss: 0.0747 - mse: 0.0090 - mae: 0.0747 - val_loss: 0.0608 - val_mse: 0.0060 - val_mae: 0.0608 - lr: 1.0000e-06\n",
      "Epoch 29/512\n",
      "127/127 [==============================] - 32s 250ms/step - loss: 0.0761 - mse: 0.0092 - mae: 0.0761 - val_loss: 0.0611 - val_mse: 0.0060 - val_mae: 0.0611 - lr: 1.0000e-06\n",
      "Epoch 30/512\n",
      "127/127 [==============================] - 32s 249ms/step - loss: 0.0739 - mse: 0.0090 - mae: 0.0739 - val_loss: 0.0610 - val_mse: 0.0060 - val_mae: 0.0610 - lr: 1.0000e-06\n",
      "Epoch 31/512\n",
      "127/127 [==============================] - 32s 250ms/step - loss: 0.0767 - mse: 0.0096 - mae: 0.0767 - val_loss: 0.0608 - val_mse: 0.0060 - val_mae: 0.0608 - lr: 1.0000e-06\n",
      "Epoch 32/512\n",
      "127/127 [==============================] - 32s 250ms/step - loss: 0.0730 - mse: 0.0087 - mae: 0.0730 - val_loss: 0.0609 - val_mse: 0.0060 - val_mae: 0.0609 - lr: 1.0000e-06\n",
      "Epoch 33/512\n",
      "127/127 [==============================] - 32s 249ms/step - loss: 0.0754 - mse: 0.0092 - mae: 0.0754 - val_loss: 0.0608 - val_mse: 0.0060 - val_mae: 0.0608 - lr: 1.0000e-06\n",
      "Epoch 34/512\n",
      "127/127 [==============================] - 32s 250ms/step - loss: 0.0743 - mse: 0.0088 - mae: 0.0743 - val_loss: 0.0605 - val_mse: 0.0060 - val_mae: 0.0605 - lr: 1.0000e-06\n",
      "Epoch 35/512\n",
      "127/127 [==============================] - 32s 249ms/step - loss: 0.0748 - mse: 0.0091 - mae: 0.0748 - val_loss: 0.0607 - val_mse: 0.0060 - val_mae: 0.0607 - lr: 1.0000e-06\n",
      "Epoch 36/512\n",
      "127/127 [==============================] - 32s 249ms/step - loss: 0.0773 - mse: 0.0097 - mae: 0.0773 - val_loss: 0.0607 - val_mse: 0.0060 - val_mae: 0.0607 - lr: 1.0000e-06\n",
      "Epoch 37/512\n",
      "127/127 [==============================] - 32s 250ms/step - loss: 0.0748 - mse: 0.0091 - mae: 0.0748 - val_loss: 0.0606 - val_mse: 0.0060 - val_mae: 0.0606 - lr: 1.0000e-06\n",
      "Epoch 38/512\n",
      "127/127 [==============================] - 32s 250ms/step - loss: 0.0780 - mse: 0.0100 - mae: 0.0780 - val_loss: 0.0608 - val_mse: 0.0060 - val_mae: 0.0608 - lr: 1.0000e-06\n",
      "Epoch 39/512\n",
      "127/127 [==============================] - 32s 250ms/step - loss: 0.0731 - mse: 0.0088 - mae: 0.0731 - val_loss: 0.0609 - val_mse: 0.0060 - val_mae: 0.0609 - lr: 1.0000e-06\n",
      "Epoch 40/512\n",
      "127/127 [==============================] - 32s 249ms/step - loss: 0.0745 - mse: 0.0089 - mae: 0.0745 - val_loss: 0.0611 - val_mse: 0.0060 - val_mae: 0.0611 - lr: 1.0000e-06\n",
      "Epoch 41/512\n",
      "127/127 [==============================] - 32s 249ms/step - loss: 0.0740 - mse: 0.0088 - mae: 0.0740 - val_loss: 0.0607 - val_mse: 0.0060 - val_mae: 0.0607 - lr: 1.0000e-06\n",
      "Epoch 42/512\n",
      "127/127 [==============================] - 32s 249ms/step - loss: 0.0747 - mse: 0.0090 - mae: 0.0747 - val_loss: 0.0607 - val_mse: 0.0060 - val_mae: 0.0607 - lr: 1.0000e-06\n",
      "Epoch 43/512\n",
      "127/127 [==============================] - 32s 250ms/step - loss: 0.0745 - mse: 0.0089 - mae: 0.0745 - val_loss: 0.0608 - val_mse: 0.0060 - val_mae: 0.0608 - lr: 1.0000e-06\n",
      "Epoch 44/512\n",
      "127/127 [==============================] - 975s 8s/step - loss: 0.0736 - mse: 0.0086 - mae: 0.0736 - val_loss: 0.0607 - val_mse: 0.0060 - val_mae: 0.0607 - lr: 1.0000e-06\n",
      "Epoch 45/512\n",
      "127/127 [==============================] - 966s 8s/step - loss: 0.0772 - mse: 0.0095 - mae: 0.0772 - val_loss: 0.0609 - val_mse: 0.0060 - val_mae: 0.0609 - lr: 1.0000e-06\n",
      "Epoch 46/512\n",
      "127/127 [==============================] - 31s 247ms/step - loss: 0.0718 - mse: 0.0085 - mae: 0.0718 - val_loss: 0.0608 - val_mse: 0.0060 - val_mae: 0.0608 - lr: 1.0000e-06\n",
      "Epoch 47/512\n",
      "127/127 [==============================] - 1011s 8s/step - loss: 0.0774 - mse: 0.0098 - mae: 0.0774 - val_loss: 0.0612 - val_mse: 0.0060 - val_mae: 0.0612 - lr: 1.0000e-06\n",
      "Epoch 48/512\n",
      "127/127 [==============================] - 31s 248ms/step - loss: 0.0755 - mse: 0.0091 - mae: 0.0755 - val_loss: 0.0612 - val_mse: 0.0060 - val_mae: 0.0612 - lr: 1.0000e-06\n",
      "Epoch 49/512\n",
      "127/127 [==============================] - 32s 248ms/step - loss: 0.0737 - mse: 0.0089 - mae: 0.0737 - val_loss: 0.0608 - val_mse: 0.0060 - val_mae: 0.0608 - lr: 1.0000e-06\n",
      "Epoch 50/512\n",
      "127/127 [==============================] - 995s 8s/step - loss: 0.0742 - mse: 0.0089 - mae: 0.0742 - val_loss: 0.0608 - val_mse: 0.0060 - val_mae: 0.0608 - lr: 1.0000e-06\n",
      "Epoch 51/512\n",
      "127/127 [==============================] - 31s 248ms/step - loss: 0.0744 - mse: 0.0088 - mae: 0.0744 - val_loss: 0.0606 - val_mse: 0.0060 - val_mae: 0.0606 - lr: 1.0000e-06\n",
      "Epoch 52/512\n",
      "127/127 [==============================] - 32s 248ms/step - loss: 0.0773 - mse: 0.0096 - mae: 0.0773 - val_loss: 0.0610 - val_mse: 0.0060 - val_mae: 0.0610 - lr: 1.0000e-06\n",
      "Epoch 53/512\n",
      "127/127 [==============================] - 339s 3s/step - loss: 0.0767 - mse: 0.0095 - mae: 0.0767 - val_loss: 0.0607 - val_mse: 0.0060 - val_mae: 0.0607 - lr: 1.0000e-06\n",
      "Epoch 54/512\n",
      "127/127 [==============================] - 31s 248ms/step - loss: 0.0747 - mse: 0.0091 - mae: 0.0747 - val_loss: 0.0608 - val_mse: 0.0060 - val_mae: 0.0608 - lr: 1.0000e-06\n",
      "Epoch 55/512\n",
      "127/127 [==============================] - 999s 8s/step - loss: 0.0760 - mse: 0.0094 - mae: 0.0760 - val_loss: 0.0607 - val_mse: 0.0060 - val_mae: 0.0607 - lr: 1.0000e-06\n",
      "Epoch 56/512\n",
      "127/127 [==============================] - 31s 248ms/step - loss: 0.0752 - mse: 0.0091 - mae: 0.0752 - val_loss: 0.0610 - val_mse: 0.0060 - val_mae: 0.0610 - lr: 1.0000e-06\n",
      "Epoch 57/512\n",
      "127/127 [==============================] - 32s 253ms/step - loss: 0.0747 - mse: 0.0091 - mae: 0.0747 - val_loss: 0.0608 - val_mse: 0.0060 - val_mae: 0.0608 - lr: 1.0000e-06\n",
      "Epoch 58/512\n",
      "127/127 [==============================] - 853s 7s/step - loss: 0.0742 - mse: 0.0089 - mae: 0.0742 - val_loss: 0.0608 - val_mse: 0.0060 - val_mae: 0.0608 - lr: 1.0000e-06\n",
      "Epoch 59/512\n",
      "127/127 [==============================] - 32s 251ms/step - loss: 0.0739 - mse: 0.0090 - mae: 0.0739 - val_loss: 0.0608 - val_mse: 0.0060 - val_mae: 0.0608 - lr: 1.0000e-06\n",
      "Epoch 60/512\n",
      "106/127 [========================>.....] - ETA: 5s - loss: 0.0760 - mse: 0.0094 - mae: 0.0760"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def dilation2d(x, st_element, strides, padding,rates=(1, 1)):\n",
    "    \"\"\"\n",
    "\n",
    "    From MORPHOLAYERS\n",
    "\n",
    "    Basic Dilation Operator\n",
    "    :param st_element: Nonflat structuring element\n",
    "    :strides: strides as classical convolutional layers\n",
    "    :padding: padding as classical convolutional layers\n",
    "    :rates: rates as classical convolutional layers\n",
    "    \"\"\"\n",
    "    x = tf.nn.dilation2d(x, st_element, (1, ) + strides + (1, ),padding.upper(),\"NHWC\",(1,)+rates+(1,))\n",
    "    return x\n",
    "\n",
    "class DepthwiseDilation2D(Layer):\n",
    "    '''\n",
    "    Depthwise Dilation 2D Layer: Depthwise Dilation for now assuming channel last\n",
    "    '''\n",
    "    def __init__(self, kernel_size,depth_multiplier=1, strides=(1, 1),padding='same', dilation_rate=(1,1), kernel_initializer=tf.keras.initializers.RandomUniform(minval=-1., maxval=0.),\n",
    "    kernel_constraint=None,kernel_regularization=None,**kwargs):\n",
    "        super(DepthwiseDilation2D, self).__init__(**kwargs)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.depth_multiplier= depth_multiplier\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.rates=dilation_rate\n",
    "        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n",
    "        self.kernel_constraint = tf.keras.constraints.get(kernel_constraint)\n",
    "        self.kernel_regularization = tf.keras.regularizers.get(kernel_regularization)\n",
    "        # for we are assuming channel last\n",
    "        self.channel_axis = -1\n",
    "\n",
    "        # self.output_dim = output_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if input_shape[self.channel_axis] is None:\n",
    "            raise ValueError('The channel dimension of the inputs '\n",
    "                             'should be defined. Found `None`.')\n",
    "\n",
    "        input_dim = input_shape[self.channel_axis]\n",
    "        kernel_shape = self.kernel_size + (input_dim,self.depth_multiplier)\n",
    "        self.kernel2D = self.add_weight(shape=kernel_shape,\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel2D',constraint =self.kernel_constraint,regularizer=self.kernel_regularization)\n",
    "        super(DepthwiseDilation2D, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        res=[]\n",
    "        for di in range(self.depth_multiplier):\n",
    "            H=tf.nn.dilation2d(x,self.kernel2D[:,:,:,di],strides=(1, ) + self.strides + (1, ),padding=self.padding.upper(),data_format=\"NHWC\",dilations=(1,)+self.rates+(1,))\n",
    "            res.append(H)\n",
    "        return tf.concat(res,axis=-1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "\n",
    "        space = input_shape[1:-1]\n",
    "        new_space = []\n",
    "        for i in range(len(space)):\n",
    "            new_dim = conv_utils.conv_output_length(\n",
    "                space[i],\n",
    "                self.kernel_size[i],\n",
    "                padding=self.padding,\n",
    "                stride=self.strides[i],\n",
    "                dilation=self.rates[i])\n",
    "            new_space.append(new_dim)\n",
    "\n",
    "        return (input_shape[0],) + tuple(new_space) + (self.depth_multiplier,)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'num_filters': self.num_filters,\n",
    "            'kernel_size': self.kernel_size,\n",
    "            'depth_multiplier': self.depth_multiplier,\n",
    "            'strides': self.strides,\n",
    "            'padding': self.padding,\n",
    "            'dilation_rate': self.rates,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "\n",
    "xinput = layers.Input(shape=(IMG_SIZE, IMG_SIZE, CHANNELS))\n",
    "xconv=xinput\n",
    "for i in range(NLAYERS):\n",
    "    xconv = layers.Conv2D(NFILTERS,(3,3),padding='same',activation='relu')(xconv)\n",
    "if NLAYERS>0:\n",
    "    xconv = layers.Conv2D(NFILTERS//SHRINK,(1,1),padding='same',activation='relu')(xconv)\n",
    "    xconv = layers.DepthwiseConv2D((KSIZE,KSIZE),depth_multiplier=SHRINK,padding='same')(xconv)\n",
    "else:\n",
    "    xconv = layers.DepthwiseConv2D((KSIZE,KSIZE),depth_multiplier=NFILTERS,padding='same')(xconv)\n",
    "xfeatures=layers.GlobalAveragePooling2D()(xconv)\n",
    "xfeatures=layers.BatchNormalization()(xfeatures)\n",
    "xfeatures=layers.Dense(SUBSPACE,'relu')(xfeatures)\n",
    "xfeatures=layers.Dense(SUBSPACE)(xfeatures)\n",
    "xend=layers.Dense(1,activation='sigmoid')(xfeatures)\n",
    "modelDWConv=tf.keras.Model(xinput,xend)\n",
    "modelDWConv.summary()\n",
    "print(modelDWConv.count_params())\n",
    "\n",
    "CBDW=[tf.keras.callbacks.EarlyStopping(patience=PATIENCE_ES,restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=PATIENCE_RP,min_lr=1e-6),\n",
    "#    tf.keras.callbacks.CSVLogger(dir_autosave_model_stat+'DWConv', separator=',', append=False)\n",
    "   ]\n",
    "modelDWConv.compile(loss=\"mae\", optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate), metrics=[\"mse\",\"mae\"])\n",
    "histDWConv=modelDWConv.fit(listIm, listY, batch_size=batch_size, epochs=epochs,callbacks=CBDW,validation_data=(listImVal, listYVal))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "xinput = layers.Input(shape=(IMG_SIZE, IMG_SIZE, CHANNELS))\n",
    "xconv=xinput\n",
    "for i in range(NLAYERS):\n",
    "    xconv = layers.Conv2D(NFILTERS,(3,3),padding='same',activation='relu')(xconv)\n",
    "xconv=layers.Conv2D(NFILTERS,(KSIZE,KSIZE),use_bias=False,padding='same')(xconv)\n",
    "xfeatures=layers.GlobalAveragePooling2D()(xconv)\n",
    "xfeatures=layers.BatchNormalization()(xfeatures)\n",
    "xfeatures=layers.Dense(SUBSPACE,'relu')(xfeatures)\n",
    "xfeatures=layers.Dense(SUBSPACE)(xfeatures)\n",
    "xend=layers.Dense(1,activation='sigmoid')(xfeatures)\n",
    "modelConv=tf.keras.Model(xinput,xend)\n",
    "modelConv.summary()\n",
    "print(modelConv.count_params())\n",
    "\n",
    "CB1=[tf.keras.callbacks.EarlyStopping(patience=PATIENCE_ES,restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=PATIENCE_RP,min_lr=1e-6),\n",
    "#    tf.keras.callbacks.CSVLogger(dir_autosave_model_stat+'Conv', separator=',', append=False)\n",
    "   ]\n",
    "modelConv.compile(loss=\"mae\", optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate), metrics=[\"mse\",\"mae\"])\n",
    "histConv=modelConv.fit(listIm, listY, batch_size=batch_size, epochs=epochs,callbacks=CB1,validation_data=(listImVal, listYVal))\n",
    "\n",
    "\n",
    "xinput = layers.Input(shape=(IMG_SIZE, IMG_SIZE, CHANNELS))\n",
    "xconv=xinput\n",
    "for i in range(NLAYERS):\n",
    "    xconv = layers.Conv2D(NFILTERS,(3,3),padding='same',activation='relu')(xconv)\n",
    "if NLAYERS>0:\n",
    "    xconv = layers.Conv2D(NFILTERS//SHRINK,(1,1),padding='same',activation='relu')(xconv)\n",
    "    xconv = DepthwiseDilation2D((KSIZE,KSIZE),depth_multiplier=SHRINK,padding='same')(xconv)\n",
    "else:\n",
    "    xconv = DepthwiseDilation2D((KSIZE,KSIZE),depth_multiplier=NFILTERS,padding='same')(xconv)\n",
    "xfeatures=layers.GlobalAveragePooling2D()(xconv)\n",
    "xfeatures=layers.BatchNormalization()(xfeatures)\n",
    "xfeatures=layers.Dense(SUBSPACE,activation='relu')(xfeatures)\n",
    "xfeatures=layers.Dense(SUBSPACE)(xfeatures)\n",
    "xend=layers.Dense(1,activation='sigmoid')(xfeatures)\n",
    "modelDil=tf.keras.Model(xinput,xend)\n",
    "modelDil.summary()\n",
    "print(modelDil.count_params())\n",
    "\n",
    "CB2=[tf.keras.callbacks.EarlyStopping(patience=PATIENCE_ES,restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=PATIENCE_RP,min_lr=1e-6),\n",
    "#    tf.keras.callbacks.CSVLogger(dir_autosave_model_stat+'Dil', separator=',', append=False)\n",
    "   ]\n",
    "modelDil.compile(loss=\"mae\", optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate), metrics=[\"mse\",\"mae\"])\n",
    "histDil=modelDil.fit(listIm, listY, batch_size=batch_size, epochs=epochs,callbacks=CB2,validation_data=(listImVal, listYVal))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de97632-5fd4-42af-b98c-256c39e2fa3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0b9b59-e1f8-4f54-9dfe-2cdc47d9f613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
