{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49f1e2c1-b6cd-4fb0-908f-87b1ccee6122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n",
      "listIm.shape (4048, 128, 128)\n",
      "listImVal.shape (512, 128, 128)\n",
      "listY.shape (4048, 1)\n",
      "listYVal.shape (512, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Layer\n",
    "from skimage.io import imread\n",
    "from shutil import copyfile\n",
    "#import tensorflow_probability as tfp\n",
    "print(tf.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.gray()\n",
    "from sklearn.metrics import accuracy_score\n",
    "#import tensorflow_probability as tfp\n",
    "import random\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "\n",
    "BATCH_SIZE = int(32.)\n",
    "EPOCHS = int(512.)\n",
    "\n",
    "batch_size = BATCH_SIZE\n",
    "epochs = EPOCHS\n",
    "learning_rate=0.001\n",
    "CHANNELS=1\n",
    "NLAYERS=int(1.)\n",
    "NFILTERS=int(48.)\n",
    "KSIZE=13\n",
    "SUBSPACE=12\n",
    "PATIENCE_ES=40\n",
    "PATIENCE_RP=5\n",
    "\n",
    "\n",
    "class NeymanScott:\n",
    "    \"\"\"\n",
    "    Neyman-Scott point process using a Poisson variable for the number of parent points, uniform for\n",
    "    the number of daughter points and Pareto distribution for the distance from the daughter points to\n",
    "    the parent.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 poisson_mean: float,\n",
    "                 daughter_max: int,\n",
    "                 pareto_alpha: float,\n",
    "                 pareto_scale: float,\n",
    "                 size: (int, int)):\n",
    "        \"\"\"\n",
    "        :param poisson_mean: mean of the number of parent points\n",
    "        :param daughter_max: maximum number of daughters per parent points\n",
    "        :param pareto_alpha: alpha parameter of the Pareto distribution\n",
    "        :param pareto_scale: scale used in the Pareto distribution. This parameter is\n",
    "            applied before resizing the points from the [0, 1] interval to the size of the image.\n",
    "        :param size: rescale the output to this size\n",
    "        \"\"\"\n",
    "        self.poisson_mean = poisson_mean\n",
    "        self.daughter_max = daughter_max\n",
    "        self.pareto_alpha = pareto_alpha\n",
    "        self.pareto_scale = pareto_scale\n",
    "        self.size = np.array([size])\n",
    "        self.generator = np.random.Generator(np.random.PCG64())\n",
    "\n",
    "    def __call__(self):\n",
    "        num_parents = self.generator.poisson(lam=self.poisson_mean)\n",
    "        parents = self.generator.random((num_parents, 2))\n",
    "        num_daughters = self.generator.integers(1, self.daughter_max, num_parents)\n",
    "        points = np.empty((0, 2))\n",
    "\n",
    "        for i in range(num_parents):\n",
    "            # normalizes the pareto II distribution\n",
    "            dist = self.generator.pareto(self.pareto_alpha, (num_daughters[i], 1))\n",
    "            dist = (dist + 1) * self.pareto_scale\n",
    "            angle = self.generator.uniform(0., 2 * np.pi, (num_daughters[i],))\n",
    "            positions = np.stack([np.cos(angle), np.sin(angle)], 1)\n",
    "            positions *= dist\n",
    "            positions += parents[i, np.newaxis, :]\n",
    "            points = np.concatenate([points, positions])\n",
    "        # remove points outside the set [0, 1] x [0, 1]\n",
    "        valid_points = np.logical_and(\n",
    "            np.logical_and(0. <= points[:, 0], points[:, 0] <= 1.),\n",
    "            np.logical_and(0. <= points[:, 1], points[:, 1] <= 1.)\n",
    "        )\n",
    "        points = points[valid_points, :]\n",
    "        # scale to the image size\n",
    "        points = points * self.size\n",
    "        return points\n",
    "\n",
    "\n",
    "\n",
    "NSAMPLES_TRAINING=2024*2\n",
    "IMG_SIZE=128\n",
    "poisson_mean=100\n",
    "daughter_max=50\n",
    "pareto_scale=.02\n",
    "pareto_alpha=1. #GENERATION ON IT\n",
    "gen = NeymanScott(poisson_mean, daughter_max, pareto_alpha, pareto_scale, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "generatedata=0\n",
    "if generatedata==1:\n",
    "    listIm=[]\n",
    "    listY=[]\n",
    "    for i in range(NSAMPLES_TRAINING):\n",
    "        pareto_alpha=gen.generator.random(1)*10\n",
    "        gen = NeymanScott(poisson_mean, daughter_max, pareto_alpha, pareto_scale, (IMG_SIZE, IMG_SIZE))\n",
    "        points = gen()\n",
    "        I=np.zeros([IMG_SIZE,IMG_SIZE])\n",
    "        I[np.int64(np.floor(points[:, 0])), np.int64(np.floor(points[:, 1]))]=1\n",
    "        listIm.append(I)\n",
    "        listY.append(pareto_alpha)\n",
    "    listIm=np.stack(listIm)\n",
    "    listY=np.stack(listY)\n",
    "\n",
    "    NSAMPLES_VALIDATION=512\n",
    "    listImVal=[]\n",
    "    listYVal=[]\n",
    "    for i in range(NSAMPLES_VALIDATION):\n",
    "        pareto_alpha=gen.generator.random(1)*10\n",
    "        gen = NeymanScott(poisson_mean, daughter_max, pareto_alpha, pareto_scale, (IMG_SIZE, IMG_SIZE))\n",
    "        points = gen()\n",
    "        I=np.zeros([IMG_SIZE,IMG_SIZE])\n",
    "        I[np.int64(np.floor(points[:, 0])), np.int64(np.floor(points[:, 1]))]=1\n",
    "        listImVal.append(I)\n",
    "        listYVal.append(pareto_alpha)\n",
    "\n",
    "    listImVal=np.stack(listImVal)\n",
    "    listYVal=np.stack(listYVal)\n",
    "    np.save('listIm.npy',listIm)\n",
    "    np.save('listImVal.npy',listImVal)\n",
    "    np.save('listY.npy',listY)\n",
    "    np.save('listYVal.npy',listYVal)\n",
    "else:\n",
    "    listIm=np.load('listIm.npy')\n",
    "    listImVal=np.load('listImVal.npy')\n",
    "    listY=np.load('listY.npy')\n",
    "    listYVal=np.load('listYVal.npy')\n",
    "\n",
    "print('listIm.shape',listIm.shape)\n",
    "print('listImVal.shape',listImVal.shape)\n",
    "print('listY.shape',listY.shape)\n",
    "print('listYVal.shape',listYVal.shape)\n",
    "\n",
    "listY=listY/9\n",
    "listYVal=listYVal/9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fdff72-ce5c-4e78-b214-887827dc0df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 128, 128, 1)]     0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 128, 128, 48)      8112      \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 128, 128, 48)      20784     \n",
      "                                                                 \n",
      " global_average_pooling2d_2  (None, 48)                0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 48)                192       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 12)                588       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 12)                156       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29845 (116.58 KB)\n",
      "Trainable params: 29749 (116.21 KB)\n",
      "Non-trainable params: 96 (384.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29845\n",
      "Epoch 1/512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 16:25:59.368643: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/127 [==============================] - ETA: 0s - loss: 0.1182 - mse: 0.0229 - mae: 0.1182"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 16:26:11.136580: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/127 [==============================] - 13s 61ms/step - loss: 0.1182 - mse: 0.0229 - mae: 0.1182 - val_loss: 0.2860 - val_mse: 0.1182 - val_mae: 0.2860 - lr: 0.0010\n",
      "Epoch 2/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0924 - mse: 0.0142 - mae: 0.0924 - val_loss: 0.2827 - val_mse: 0.1154 - val_mae: 0.2827 - lr: 0.0010\n",
      "Epoch 3/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0857 - mse: 0.0115 - mae: 0.0857 - val_loss: 0.2510 - val_mse: 0.0928 - val_mae: 0.2510 - lr: 0.0010\n",
      "Epoch 4/512\n",
      "127/127 [==============================] - 6s 43ms/step - loss: 0.0809 - mse: 0.0104 - mae: 0.0809 - val_loss: 0.1258 - val_mse: 0.0244 - val_mae: 0.1258 - lr: 0.0010\n",
      "Epoch 5/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0789 - mse: 0.0100 - mae: 0.0789 - val_loss: 0.0890 - val_mse: 0.0128 - val_mae: 0.0890 - lr: 0.0010\n",
      "Epoch 6/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0806 - mse: 0.0105 - mae: 0.0806 - val_loss: 0.0718 - val_mse: 0.0085 - val_mae: 0.0718 - lr: 0.0010\n",
      "Epoch 7/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0816 - mse: 0.0106 - mae: 0.0816 - val_loss: 0.2366 - val_mse: 0.0730 - val_mae: 0.2366 - lr: 0.0010\n",
      "Epoch 8/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0845 - mse: 0.0114 - mae: 0.0845 - val_loss: 0.1087 - val_mse: 0.0170 - val_mae: 0.1087 - lr: 0.0010\n",
      "Epoch 9/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0782 - mse: 0.0097 - mae: 0.0782 - val_loss: 0.1424 - val_mse: 0.0262 - val_mae: 0.1424 - lr: 0.0010\n",
      "Epoch 10/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0829 - mse: 0.0110 - mae: 0.0829 - val_loss: 0.0745 - val_mse: 0.0085 - val_mae: 0.0745 - lr: 0.0010\n",
      "Epoch 11/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0887 - mse: 0.0131 - mae: 0.0887 - val_loss: 0.1203 - val_mse: 0.0220 - val_mae: 0.1203 - lr: 0.0010\n",
      "Epoch 12/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0776 - mse: 0.0094 - mae: 0.0776 - val_loss: 0.0639 - val_mse: 0.0068 - val_mae: 0.0639 - lr: 1.0000e-04\n",
      "Epoch 13/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0771 - mse: 0.0094 - mae: 0.0771 - val_loss: 0.0831 - val_mse: 0.0109 - val_mae: 0.0831 - lr: 1.0000e-04\n",
      "Epoch 14/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0716 - mse: 0.0083 - mae: 0.0716 - val_loss: 0.0736 - val_mse: 0.0085 - val_mae: 0.0736 - lr: 1.0000e-04\n",
      "Epoch 15/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0747 - mse: 0.0090 - mae: 0.0747 - val_loss: 0.0660 - val_mse: 0.0067 - val_mae: 0.0660 - lr: 1.0000e-04\n",
      "Epoch 16/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0763 - mse: 0.0093 - mae: 0.0763 - val_loss: 0.0643 - val_mse: 0.0065 - val_mae: 0.0643 - lr: 1.0000e-04\n",
      "Epoch 17/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0738 - mse: 0.0087 - mae: 0.0738 - val_loss: 0.0633 - val_mse: 0.0063 - val_mae: 0.0633 - lr: 1.0000e-04\n",
      "Epoch 18/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0743 - mse: 0.0088 - mae: 0.0743 - val_loss: 0.0882 - val_mse: 0.0116 - val_mae: 0.0882 - lr: 1.0000e-04\n",
      "Epoch 19/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0744 - mse: 0.0088 - mae: 0.0744 - val_loss: 0.0695 - val_mse: 0.0074 - val_mae: 0.0695 - lr: 1.0000e-04\n",
      "Epoch 20/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0749 - mse: 0.0088 - mae: 0.0749 - val_loss: 0.0691 - val_mse: 0.0074 - val_mae: 0.0691 - lr: 1.0000e-04\n",
      "Epoch 21/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0737 - mse: 0.0087 - mae: 0.0737 - val_loss: 0.0702 - val_mse: 0.0075 - val_mae: 0.0702 - lr: 1.0000e-04\n",
      "Epoch 22/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0757 - mse: 0.0090 - mae: 0.0757 - val_loss: 0.0656 - val_mse: 0.0069 - val_mae: 0.0656 - lr: 1.0000e-04\n",
      "Epoch 23/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0731 - mse: 0.0085 - mae: 0.0731 - val_loss: 0.0637 - val_mse: 0.0064 - val_mae: 0.0637 - lr: 1.0000e-05\n",
      "Epoch 24/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0780 - mse: 0.0096 - mae: 0.0780 - val_loss: 0.0643 - val_mse: 0.0065 - val_mae: 0.0643 - lr: 1.0000e-05\n",
      "Epoch 25/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0734 - mse: 0.0085 - mae: 0.0734 - val_loss: 0.0639 - val_mse: 0.0064 - val_mae: 0.0639 - lr: 1.0000e-05\n",
      "Epoch 26/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0763 - mse: 0.0093 - mae: 0.0763 - val_loss: 0.0639 - val_mse: 0.0064 - val_mae: 0.0639 - lr: 1.0000e-05\n",
      "Epoch 27/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0735 - mse: 0.0085 - mae: 0.0735 - val_loss: 0.0637 - val_mse: 0.0063 - val_mae: 0.0637 - lr: 1.0000e-05\n",
      "Epoch 28/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0750 - mse: 0.0089 - mae: 0.0750 - val_loss: 0.0635 - val_mse: 0.0063 - val_mae: 0.0635 - lr: 1.0000e-06\n",
      "Epoch 29/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0743 - mse: 0.0087 - mae: 0.0743 - val_loss: 0.0636 - val_mse: 0.0063 - val_mae: 0.0636 - lr: 1.0000e-06\n",
      "Epoch 30/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0721 - mse: 0.0083 - mae: 0.0721 - val_loss: 0.0635 - val_mse: 0.0063 - val_mae: 0.0635 - lr: 1.0000e-06\n",
      "Epoch 31/512\n",
      "127/127 [==============================] - 6s 45ms/step - loss: 0.0740 - mse: 0.0087 - mae: 0.0740 - val_loss: 0.0638 - val_mse: 0.0064 - val_mae: 0.0638 - lr: 1.0000e-06\n",
      "Epoch 32/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0742 - mse: 0.0086 - mae: 0.0742 - val_loss: 0.0637 - val_mse: 0.0064 - val_mae: 0.0637 - lr: 1.0000e-06\n",
      "Epoch 33/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0728 - mse: 0.0084 - mae: 0.0728 - val_loss: 0.0637 - val_mse: 0.0064 - val_mae: 0.0637 - lr: 1.0000e-06\n",
      "Epoch 34/512\n",
      "127/127 [==============================] - 6s 45ms/step - loss: 0.0732 - mse: 0.0086 - mae: 0.0732 - val_loss: 0.0637 - val_mse: 0.0064 - val_mae: 0.0637 - lr: 1.0000e-06\n",
      "Epoch 35/512\n",
      "127/127 [==============================] - 6s 45ms/step - loss: 0.0732 - mse: 0.0084 - mae: 0.0732 - val_loss: 0.0637 - val_mse: 0.0064 - val_mae: 0.0637 - lr: 1.0000e-06\n",
      "Epoch 36/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0770 - mse: 0.0093 - mae: 0.0770 - val_loss: 0.0636 - val_mse: 0.0063 - val_mae: 0.0636 - lr: 1.0000e-06\n",
      "Epoch 37/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0745 - mse: 0.0088 - mae: 0.0745 - val_loss: 0.0637 - val_mse: 0.0063 - val_mae: 0.0637 - lr: 1.0000e-06\n",
      "Epoch 38/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0727 - mse: 0.0085 - mae: 0.0727 - val_loss: 0.0637 - val_mse: 0.0064 - val_mae: 0.0637 - lr: 1.0000e-06\n",
      "Epoch 39/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0768 - mse: 0.0093 - mae: 0.0768 - val_loss: 0.0637 - val_mse: 0.0063 - val_mae: 0.0637 - lr: 1.0000e-06\n",
      "Epoch 40/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0762 - mse: 0.0092 - mae: 0.0762 - val_loss: 0.0636 - val_mse: 0.0063 - val_mae: 0.0636 - lr: 1.0000e-06\n",
      "Epoch 41/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0772 - mse: 0.0094 - mae: 0.0772 - val_loss: 0.0637 - val_mse: 0.0064 - val_mae: 0.0637 - lr: 1.0000e-06\n",
      "Epoch 42/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0727 - mse: 0.0085 - mae: 0.0727 - val_loss: 0.0636 - val_mse: 0.0063 - val_mae: 0.0636 - lr: 1.0000e-06\n",
      "Epoch 43/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0762 - mse: 0.0091 - mae: 0.0762 - val_loss: 0.0636 - val_mse: 0.0063 - val_mae: 0.0636 - lr: 1.0000e-06\n",
      "Epoch 44/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0743 - mse: 0.0086 - mae: 0.0743 - val_loss: 0.0637 - val_mse: 0.0063 - val_mae: 0.0637 - lr: 1.0000e-06\n",
      "Epoch 45/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0738 - mse: 0.0086 - mae: 0.0738 - val_loss: 0.0635 - val_mse: 0.0063 - val_mae: 0.0635 - lr: 1.0000e-06\n",
      "Epoch 46/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0751 - mse: 0.0089 - mae: 0.0751 - val_loss: 0.0637 - val_mse: 0.0063 - val_mae: 0.0637 - lr: 1.0000e-06\n",
      "Epoch 47/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0736 - mse: 0.0085 - mae: 0.0736 - val_loss: 0.0636 - val_mse: 0.0063 - val_mae: 0.0636 - lr: 1.0000e-06\n",
      "Epoch 48/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0757 - mse: 0.0090 - mae: 0.0757 - val_loss: 0.0638 - val_mse: 0.0064 - val_mae: 0.0638 - lr: 1.0000e-06\n",
      "Epoch 49/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0711 - mse: 0.0081 - mae: 0.0711 - val_loss: 0.0635 - val_mse: 0.0063 - val_mae: 0.0635 - lr: 1.0000e-06\n",
      "Epoch 50/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0759 - mse: 0.0092 - mae: 0.0759 - val_loss: 0.0636 - val_mse: 0.0063 - val_mae: 0.0636 - lr: 1.0000e-06\n",
      "Epoch 51/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0732 - mse: 0.0085 - mae: 0.0732 - val_loss: 0.0636 - val_mse: 0.0063 - val_mae: 0.0636 - lr: 1.0000e-06\n",
      "Epoch 52/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0759 - mse: 0.0090 - mae: 0.0759 - val_loss: 0.0636 - val_mse: 0.0063 - val_mae: 0.0636 - lr: 1.0000e-06\n",
      "Epoch 53/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0725 - mse: 0.0083 - mae: 0.0725 - val_loss: 0.0635 - val_mse: 0.0063 - val_mae: 0.0635 - lr: 1.0000e-06\n",
      "Epoch 54/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0737 - mse: 0.0086 - mae: 0.0737 - val_loss: 0.0636 - val_mse: 0.0063 - val_mae: 0.0636 - lr: 1.0000e-06\n",
      "Epoch 55/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0748 - mse: 0.0087 - mae: 0.0748 - val_loss: 0.0637 - val_mse: 0.0064 - val_mae: 0.0637 - lr: 1.0000e-06\n",
      "Epoch 56/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0741 - mse: 0.0086 - mae: 0.0741 - val_loss: 0.0637 - val_mse: 0.0063 - val_mae: 0.0637 - lr: 1.0000e-06\n",
      "Epoch 57/512\n",
      "127/127 [==============================] - 6s 44ms/step - loss: 0.0723 - mse: 0.0083 - mae: 0.0723 - val_loss: 0.0638 - val_mse: 0.0064 - val_mae: 0.0638 - lr: 1.0000e-06\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 128, 128, 1)]     0         \n",
      "                                                                 \n",
      " depthwise_dilation2d (Dept  (None, 128, 128, 48)      8112      \n",
      " hwiseDilation2D)                                                \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 128, 128, 48)      20784     \n",
      "                                                                 \n",
      " global_average_pooling2d_3  (None, 48)                0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 48)                192       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 12)                588       \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 12)                156       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29845 (116.58 KB)\n",
      "Trainable params: 29749 (116.21 KB)\n",
      "Non-trainable params: 96 (384.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29845\n",
      "Epoch 1/512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 16:31:26.216602: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/127 [==============================] - ETA: 0s - loss: 0.1290 - mse: 0.0310 - mae: 0.1290"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 16:33:56.613492: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/127 [==============================] - 161s 1s/step - loss: 0.1290 - mse: 0.0310 - mae: 0.1290 - val_loss: 0.2572 - val_mse: 0.0898 - val_mae: 0.2572 - lr: 0.0010\n",
      "Epoch 2/512\n",
      " 24/127 [====>.........................] - ETA: 1:52 - loss: 0.1026 - mse: 0.0181 - mae: 0.1026"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def dilation2d(x, st_element, strides, padding,rates=(1, 1)):\n",
    "    \"\"\"\n",
    "\n",
    "    From MORPHOLAYERS\n",
    "\n",
    "    Basic Dilation Operator\n",
    "    :param st_element: Nonflat structuring element\n",
    "    :strides: strides as classical convolutional layers\n",
    "    :padding: padding as classical convolutional layers\n",
    "    :rates: rates as classical convolutional layers\n",
    "    \"\"\"\n",
    "    x = tf.nn.dilation2d(x, st_element, (1, ) + strides + (1, ),padding.upper(),\"NHWC\",(1,)+rates+(1,))\n",
    "    return x\n",
    "\n",
    "class DepthwiseDilation2D(Layer):\n",
    "    '''\n",
    "    Depthwise Dilation 2D Layer: Depthwise Dilation for now assuming channel last\n",
    "    '''\n",
    "    def __init__(self, kernel_size,depth_multiplier=1, strides=(1, 1),padding='same', dilation_rate=(1,1), kernel_initializer=tf.keras.initializers.RandomUniform(minval=-1., maxval=0.),\n",
    "    kernel_constraint=None,kernel_regularization=None,**kwargs):\n",
    "        super(DepthwiseDilation2D, self).__init__(**kwargs)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.depth_multiplier= depth_multiplier\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.rates=dilation_rate\n",
    "        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n",
    "        self.kernel_constraint = tf.keras.constraints.get(kernel_constraint)\n",
    "        self.kernel_regularization = tf.keras.regularizers.get(kernel_regularization)\n",
    "        # for we are assuming channel last\n",
    "        self.channel_axis = -1\n",
    "\n",
    "        # self.output_dim = output_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if input_shape[self.channel_axis] is None:\n",
    "            raise ValueError('The channel dimension of the inputs '\n",
    "                             'should be defined. Found `None`.')\n",
    "\n",
    "        input_dim = input_shape[self.channel_axis]\n",
    "        kernel_shape = self.kernel_size + (input_dim,self.depth_multiplier)\n",
    "        self.kernel2D = self.add_weight(shape=kernel_shape,\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel2D',constraint =self.kernel_constraint,regularizer=self.kernel_regularization)\n",
    "        super(DepthwiseDilation2D, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        res=[]\n",
    "        for di in range(self.depth_multiplier):\n",
    "            H=tf.nn.dilation2d(x,self.kernel2D[:,:,:,di],strides=(1, ) + self.strides + (1, ),padding=self.padding.upper(),data_format=\"NHWC\",dilations=(1,)+self.rates+(1,))\n",
    "            res.append(H)\n",
    "        return tf.concat(res,axis=-1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "\n",
    "        space = input_shape[1:-1]\n",
    "        new_space = []\n",
    "        for i in range(len(space)):\n",
    "            new_dim = conv_utils.conv_output_length(\n",
    "                space[i],\n",
    "                self.kernel_size[i],\n",
    "                padding=self.padding,\n",
    "                stride=self.strides[i],\n",
    "                dilation=self.rates[i])\n",
    "            new_space.append(new_dim)\n",
    "\n",
    "        return (input_shape[0],) + tuple(new_space) + (self.depth_multiplier,)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'num_filters': self.num_filters,\n",
    "            'kernel_size': self.kernel_size,\n",
    "            'depth_multiplier': self.depth_multiplier,\n",
    "            'strides': self.strides,\n",
    "            'padding': self.padding,\n",
    "            'dilation_rate': self.rates,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "\n",
    "xinput = layers.Input(shape=(IMG_SIZE, IMG_SIZE, CHANNELS))\n",
    "xconv=layers.Conv2D(NFILTERS,(KSIZE,KSIZE),use_bias=False,padding='same')(xinput)\n",
    "for i in range(NLAYERS):\n",
    "    xconv = layers.Conv2D(NFILTERS,(3,3),padding='same',activation='relu')(xconv)\n",
    "xfeatures=layers.GlobalAveragePooling2D()(xconv)\n",
    "xfeatures=layers.BatchNormalization()(xfeatures)\n",
    "xfeatures=layers.Dense(SUBSPACE,'relu')(xfeatures)\n",
    "xfeatures=layers.Dense(SUBSPACE)(xfeatures)\n",
    "xend=layers.Dense(1,activation='sigmoid')(xfeatures)\n",
    "modelConv=tf.keras.Model(xinput,xend)\n",
    "modelConv.summary()\n",
    "print(modelConv.count_params())\n",
    "\n",
    "CB1=[tf.keras.callbacks.EarlyStopping(patience=PATIENCE_ES,restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=PATIENCE_RP,min_lr=1e-6),\n",
    "#    tf.keras.callbacks.CSVLogger(dir_autosave_model_stat+'Conv', separator=',', append=False)\n",
    "   ]\n",
    "modelConv.compile(loss=\"mae\", optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), metrics=[\"mse\",\"mae\"])\n",
    "histConv=modelConv.fit(listIm, listY, batch_size=batch_size, epochs=epochs,callbacks=CB1,validation_data=(listImVal, listYVal))\n",
    "\n",
    "\n",
    "xinput = layers.Input(shape=(IMG_SIZE, IMG_SIZE, CHANNELS))\n",
    "xconv = DepthwiseDilation2D((KSIZE,KSIZE),depth_multiplier=NFILTERS,padding='same')(xinput)\n",
    "for i in range(NLAYERS):\n",
    "    xconv = layers.Conv2D(NFILTERS,(3,3),padding='same',activation='relu')(xconv)\n",
    "xfeatures=layers.GlobalAveragePooling2D()(xconv)\n",
    "xfeatures=layers.BatchNormalization()(xfeatures)\n",
    "xfeatures=layers.Dense(SUBSPACE,activation='relu')(xfeatures)\n",
    "xfeatures=layers.Dense(SUBSPACE)(xfeatures)\n",
    "xend=layers.Dense(1,activation='sigmoid')(xfeatures)\n",
    "modelDil=tf.keras.Model(xinput,xend)\n",
    "modelDil.summary()\n",
    "print(modelDil.count_params())\n",
    "\n",
    "CB2=[tf.keras.callbacks.EarlyStopping(patience=PATIENCE_ES,restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=PATIENCE_RP,min_lr=1e-6),\n",
    "#    tf.keras.callbacks.CSVLogger(dir_autosave_model_stat+'Dil', separator=',', append=False)\n",
    "   ]\n",
    "modelDil.compile(loss=\"mae\", optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), metrics=[\"mse\",\"mae\"])\n",
    "histDil=modelDil.fit(listIm, listY, batch_size=batch_size, epochs=epochs,callbacks=CB2,validation_data=(listImVal, listYVal))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de97632-5fd4-42af-b98c-256c39e2fa3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0b9b59-e1f8-4f54-9dfe-2cdc47d9f613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
