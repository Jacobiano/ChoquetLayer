{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49f1e2c1-b6cd-4fb0-908f-87b1ccee6122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version\n",
      "2.13.0\n",
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n",
      "listIm.shape (4048, 128, 128)\n",
      "listImVal.shape (512, 128, 128)\n",
      "listY.shape (4048, 1)\n",
      "listYVal.shape (512, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Layer\n",
    "from skimage.io import imread\n",
    "from shutil import copyfile\n",
    "#import tensorflow_probability as tfp\n",
    "print('Tensorflow version')\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_logical_devices())\n",
    "import matplotlib.pyplot as plt\n",
    "plt.gray()\n",
    "from sklearn.metrics import accuracy_score\n",
    "#import tensorflow_probability as tfp\n",
    "import random\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "\n",
    "BATCH_SIZE = int(32.)\n",
    "EPOCHS = int(512.)\n",
    "\n",
    "batch_size = BATCH_SIZE\n",
    "epochs = EPOCHS\n",
    "learning_rate=0.001\n",
    "CHANNELS=1\n",
    "NLAYERS=3\n",
    "SHRINK=1\n",
    "NFILTERS=48\n",
    "KSIZE=13\n",
    "SUBSPACE=12\n",
    "PATIENCE_ES=40\n",
    "PATIENCE_RP=5\n",
    "\n",
    "\n",
    "class NeymanScott:\n",
    "    \"\"\"\n",
    "    Neyman-Scott point process using a Poisson variable for the number of parent points, uniform for\n",
    "    the number of daughter points and Pareto distribution for the distance from the daughter points to\n",
    "    the parent.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 poisson_mean: float,\n",
    "                 daughter_max: int,\n",
    "                 pareto_alpha: float,\n",
    "                 pareto_scale: float,\n",
    "                 size: (int, int)):\n",
    "        \"\"\"\n",
    "        :param poisson_mean: mean of the number of parent points\n",
    "        :param daughter_max: maximum number of daughters per parent points\n",
    "        :param pareto_alpha: alpha parameter of the Pareto distribution\n",
    "        :param pareto_scale: scale used in the Pareto distribution. This parameter is\n",
    "            applied before resizing the points from the [0, 1] interval to the size of the image.\n",
    "        :param size: rescale the output to this size\n",
    "        \"\"\"\n",
    "        self.poisson_mean = poisson_mean\n",
    "        self.daughter_max = daughter_max\n",
    "        self.pareto_alpha = pareto_alpha\n",
    "        self.pareto_scale = pareto_scale\n",
    "        self.size = np.array([size])\n",
    "        self.generator = np.random.Generator(np.random.PCG64())\n",
    "\n",
    "    def __call__(self):\n",
    "        num_parents = self.generator.poisson(lam=self.poisson_mean)\n",
    "        parents = self.generator.random((num_parents, 2))\n",
    "        num_daughters = self.generator.integers(1, self.daughter_max, num_parents)\n",
    "        points = np.empty((0, 2))\n",
    "\n",
    "        for i in range(num_parents):\n",
    "            # normalizes the pareto II distribution\n",
    "            dist = self.generator.pareto(self.pareto_alpha, (num_daughters[i], 1))\n",
    "            dist = (dist + 1) * self.pareto_scale\n",
    "            angle = self.generator.uniform(0., 2 * np.pi, (num_daughters[i],))\n",
    "            positions = np.stack([np.cos(angle), np.sin(angle)], 1)\n",
    "            positions *= dist\n",
    "            positions += parents[i, np.newaxis, :]\n",
    "            points = np.concatenate([points, positions])\n",
    "        # remove points outside the set [0, 1] x [0, 1]\n",
    "        valid_points = np.logical_and(\n",
    "            np.logical_and(0. <= points[:, 0], points[:, 0] <= 1.),\n",
    "            np.logical_and(0. <= points[:, 1], points[:, 1] <= 1.)\n",
    "        )\n",
    "        points = points[valid_points, :]\n",
    "        # scale to the image size\n",
    "        points = points * self.size\n",
    "        return points\n",
    "\n",
    "\n",
    "\n",
    "NSAMPLES_TRAINING=2024*2\n",
    "IMG_SIZE=128\n",
    "\n",
    "poisson_mean=100\n",
    "daughter_max=50\n",
    "pareto_scale=.02\n",
    "pareto_alpha=1. #GENERATION ON IT\n",
    "gen = NeymanScott(poisson_mean, daughter_max, pareto_alpha, pareto_scale, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "generatedata=1\n",
    "if generatedata==1:\n",
    "    listIm=[]\n",
    "    listY=[]\n",
    "    for i in range(NSAMPLES_TRAINING):\n",
    "        pareto_alpha=gen.generator.random(1)*10\n",
    "        gen = NeymanScott(poisson_mean, daughter_max, pareto_alpha, pareto_scale, (IMG_SIZE, IMG_SIZE))\n",
    "        points = gen()\n",
    "        I=np.zeros([IMG_SIZE,IMG_SIZE])\n",
    "        I[np.int64(np.floor(points[:, 0])), np.int64(np.floor(points[:, 1]))]=1\n",
    "        listIm.append(I)\n",
    "        listY.append(pareto_alpha)\n",
    "    listIm=np.stack(listIm)\n",
    "    listY=np.stack(listY)\n",
    "\n",
    "    NSAMPLES_VALIDATION=512\n",
    "    listImVal=[]\n",
    "    listYVal=[]\n",
    "    for i in range(NSAMPLES_VALIDATION):\n",
    "        pareto_alpha=gen.generator.random(1)*10\n",
    "        gen = NeymanScott(poisson_mean, daughter_max, pareto_alpha, pareto_scale, (IMG_SIZE, IMG_SIZE))\n",
    "        points = gen()\n",
    "        I=np.zeros([IMG_SIZE,IMG_SIZE])\n",
    "        I[np.int64(np.floor(points[:, 0])), np.int64(np.floor(points[:, 1]))]=1\n",
    "        listImVal.append(I)\n",
    "        listYVal.append(pareto_alpha)\n",
    "\n",
    "    listImVal=np.stack(listImVal)\n",
    "    listYVal=np.stack(listYVal)\n",
    "    np.save('listIm.npy',listIm)\n",
    "    np.save('listImVal.npy',listImVal)\n",
    "    np.save('listY.npy',listY)\n",
    "    np.save('listYVal.npy',listYVal)\n",
    "else:\n",
    "    listIm=np.load('listIm.npy')\n",
    "    listImVal=np.load('listImVal.npy')\n",
    "    listY=np.load('listY.npy')\n",
    "    listYVal=np.load('listYVal.npy')\n",
    "\n",
    "print('listIm.shape',listIm.shape)\n",
    "print('listImVal.shape',listImVal.shape)\n",
    "print('listY.shape',listY.shape)\n",
    "print('listYVal.shape',listYVal.shape)\n",
    "\n",
    "listY=listY/9\n",
    "listYVal=listYVal/9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fdff72-ce5c-4e78-b214-887827dc0df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 128, 128, 1)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 128, 128, 48)      480       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 128, 128, 48)      20784     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 128, 128, 48)      20784     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 128, 128, 48)      2352      \n",
      "                                                                 \n",
      " depthwise_dilation2d (Dept  (None, 128, 128, 48)      8112      \n",
      " hwiseDilation2D)                                                \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 48)                0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 48)                192       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 12)                588       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 12)                156       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53461 (208.83 KB)\n",
      "Trainable params: 53365 (208.46 KB)\n",
      "Non-trainable params: 96 (384.00 Byte)\n",
      "_________________________________________________________________\n",
      "53461\n",
      "Epoch 1/512\n",
      "  6/127 [>.............................] - ETA: 20:30 - loss: 0.2496 - mse: 0.0865 - mae: 0.2496"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def dilation2d(x, st_element, strides, padding,rates=(1, 1)):\n",
    "    \"\"\"\n",
    "\n",
    "    From MORPHOLAYERS\n",
    "\n",
    "    Basic Dilation Operator\n",
    "    :param st_element: Nonflat structuring element\n",
    "    :strides: strides as classical convolutional layers\n",
    "    :padding: padding as classical convolutional layers\n",
    "    :rates: rates as classical convolutional layers\n",
    "    \"\"\"\n",
    "    x = tf.nn.dilation2d(x, st_element, (1, ) + strides + (1, ),padding.upper(),\"NHWC\",(1,)+rates+(1,))\n",
    "    return x\n",
    "\n",
    "class DepthwiseDilation2D(Layer):\n",
    "    '''\n",
    "    Depthwise Dilation 2D Layer: Depthwise Dilation for now assuming channel last\n",
    "    '''\n",
    "    def __init__(self, kernel_size,depth_multiplier=1, strides=(1, 1),padding='same', dilation_rate=(1,1), kernel_initializer=tf.keras.initializers.RandomUniform(minval=-1., maxval=0.),\n",
    "    kernel_constraint=None,kernel_regularization=None,**kwargs):\n",
    "        super(DepthwiseDilation2D, self).__init__(**kwargs)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.depth_multiplier= depth_multiplier\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.rates=dilation_rate\n",
    "        self.kernel_initializer = tf.keras.initializers.get(kernel_initializer)\n",
    "        self.kernel_constraint = tf.keras.constraints.get(kernel_constraint)\n",
    "        self.kernel_regularization = tf.keras.regularizers.get(kernel_regularization)\n",
    "        # for we are assuming channel last\n",
    "        self.channel_axis = -1\n",
    "\n",
    "        # self.output_dim = output_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if input_shape[self.channel_axis] is None:\n",
    "            raise ValueError('The channel dimension of the inputs '\n",
    "                             'should be defined. Found `None`.')\n",
    "\n",
    "        input_dim = input_shape[self.channel_axis]\n",
    "        kernel_shape = self.kernel_size + (input_dim,self.depth_multiplier)\n",
    "        self.kernel2D = self.add_weight(shape=kernel_shape,\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel2D',constraint =self.kernel_constraint,regularizer=self.kernel_regularization)\n",
    "        super(DepthwiseDilation2D, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        res=[]\n",
    "        for di in range(self.depth_multiplier):\n",
    "            H=tf.nn.dilation2d(x,self.kernel2D[:,:,:,di],strides=(1, ) + self.strides + (1, ),padding=self.padding.upper(),data_format=\"NHWC\",dilations=(1,)+self.rates+(1,))\n",
    "            res.append(H)\n",
    "        return tf.concat(res,axis=-1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "\n",
    "        space = input_shape[1:-1]\n",
    "        new_space = []\n",
    "        for i in range(len(space)):\n",
    "            new_dim = conv_utils.conv_output_length(\n",
    "                space[i],\n",
    "                self.kernel_size[i],\n",
    "                padding=self.padding,\n",
    "                stride=self.strides[i],\n",
    "                dilation=self.rates[i])\n",
    "            new_space.append(new_dim)\n",
    "\n",
    "        return (input_shape[0],) + tuple(new_space) + (self.depth_multiplier,)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'num_filters': self.num_filters,\n",
    "            'kernel_size': self.kernel_size,\n",
    "            'depth_multiplier': self.depth_multiplier,\n",
    "            'strides': self.strides,\n",
    "            'padding': self.padding,\n",
    "            'dilation_rate': self.rates,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "xinput = layers.Input(shape=(IMG_SIZE, IMG_SIZE, CHANNELS))\n",
    "xconv=xinput\n",
    "for i in range(NLAYERS):\n",
    "    xconv = layers.Conv2D(NFILTERS,(3,3),padding='same',activation='relu')(xconv)\n",
    "if NLAYERS>0:\n",
    "    xconv = layers.Conv2D(NFILTERS//SHRINK,(1,1),padding='same',activation='relu')(xconv)\n",
    "    xconv = DepthwiseDilation2D((KSIZE,KSIZE),depth_multiplier=SHRINK,padding='same')(xconv)\n",
    "else:\n",
    "    xconv = DepthwiseDilation2D((KSIZE,KSIZE),depth_multiplier=NFILTERS,padding='same')(xconv)\n",
    "xfeatures=layers.GlobalAveragePooling2D()(xconv)\n",
    "xfeatures=layers.BatchNormalization()(xfeatures)\n",
    "xfeatures=layers.Dense(SUBSPACE,activation='relu')(xfeatures)\n",
    "xfeatures=layers.Dense(SUBSPACE)(xfeatures)\n",
    "xend=layers.Dense(1,activation='sigmoid')(xfeatures)\n",
    "modelDil=tf.keras.Model(xinput,xend)\n",
    "modelDil.summary()\n",
    "print(modelDil.count_params())\n",
    "\n",
    "CB2=[tf.keras.callbacks.EarlyStopping(patience=PATIENCE_ES,restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=PATIENCE_RP,min_lr=1e-6),\n",
    "#    tf.keras.callbacks.CSVLogger(dir_autosave_model_stat+'Dil', separator=',', append=False)\n",
    "   ]\n",
    "modelDil.compile(loss=\"mae\", optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate), metrics=[\"mse\",\"mae\"])\n",
    "histDil=modelDil.fit(listIm, listY, batch_size=batch_size, epochs=epochs,callbacks=CB2,validation_data=(listImVal, listYVal))\n",
    "\n",
    "\n",
    "xinput = layers.Input(shape=(IMG_SIZE, IMG_SIZE, CHANNELS))\n",
    "xconv=xinput\n",
    "for i in range(NLAYERS):\n",
    "    xconv = layers.Conv2D(NFILTERS,(3,3),padding='same',activation='relu')(xconv)\n",
    "if NLAYERS>0:\n",
    "    xconv = layers.Conv2D(NFILTERS//SHRINK,(1,1),padding='same',activation='relu')(xconv)\n",
    "    xconv = layers.DepthwiseConv2D((KSIZE,KSIZE),depth_multiplier=SHRINK,padding='same')(xconv)\n",
    "else:\n",
    "    xconv = layers.DepthwiseConv2D((KSIZE,KSIZE),depth_multiplier=NFILTERS,padding='same',use_bias=False)(xconv)\n",
    "xfeatures=layers.GlobalAveragePooling2D()(xconv)\n",
    "xfeatures=layers.BatchNormalization()(xfeatures)\n",
    "xfeatures=layers.Dense(SUBSPACE,'relu')(xfeatures)\n",
    "xfeatures=layers.Dense(SUBSPACE)(xfeatures)\n",
    "xend=layers.Dense(1,activation='sigmoid')(xfeatures)\n",
    "modelDWConv=tf.keras.Model(xinput,xend)\n",
    "modelDWConv.summary()\n",
    "print(modelDWConv.count_params())\n",
    "\n",
    "CBDW=[tf.keras.callbacks.EarlyStopping(patience=PATIENCE_ES,restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=PATIENCE_RP,min_lr=1e-6),\n",
    "#    tf.keras.callbacks.CSVLogger(dir_autosave_model_stat+'DWConv', separator=',', append=False)\n",
    "   ]\n",
    "modelDWConv.compile(loss=\"mae\", optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate), metrics=[\"mse\",\"mae\"])\n",
    "histDWConv=modelDWConv.fit(listIm, listY, batch_size=batch_size, epochs=epochs,callbacks=CBDW,validation_data=(listImVal, listYVal))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "xinput = layers.Input(shape=(IMG_SIZE, IMG_SIZE, CHANNELS))\n",
    "xconv=xinput\n",
    "for i in range(NLAYERS):\n",
    "    xconv = layers.Conv2D(NFILTERS,(3,3),padding='same',activation='relu')(xconv)\n",
    "xconv=layers.Conv2D(NFILTERS,(KSIZE,KSIZE),use_bias=False,padding='same')(xconv)\n",
    "xfeatures=layers.GlobalAveragePooling2D()(xconv)\n",
    "xfeatures=layers.BatchNormalization()(xfeatures)\n",
    "xfeatures=layers.Dense(SUBSPACE,'relu')(xfeatures)\n",
    "xfeatures=layers.Dense(SUBSPACE)(xfeatures)\n",
    "xend=layers.Dense(1,activation='sigmoid')(xfeatures)\n",
    "modelConv=tf.keras.Model(xinput,xend)\n",
    "modelConv.summary()\n",
    "print(modelConv.count_params())\n",
    "\n",
    "CB1=[tf.keras.callbacks.EarlyStopping(patience=PATIENCE_ES,restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=PATIENCE_RP,min_lr=1e-6),\n",
    "#    tf.keras.callbacks.CSVLogger(dir_autosave_model_stat+'Conv', separator=',', append=False)\n",
    "   ]\n",
    "modelConv.compile(loss=\"mae\", optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate), metrics=[\"mse\",\"mae\"])\n",
    "histConv=modelConv.fit(listIm, listY, batch_size=batch_size, epochs=epochs,callbacks=CB1,validation_data=(listImVal, listYVal))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e86635b-6545-4968-a8f7-ac01474085e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b66ff4d-fde9-4fb9-ae34-c27b0aa4a2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de97632-5fd4-42af-b98c-256c39e2fa3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0b9b59-e1f8-4f54-9dfe-2cdc47d9f613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
